{
  "hash": "a01cc359c6c694c1b0a2561e8ec16758",
  "result": {
    "markdown": "---\ntitle:  '{{< animate fadeInDown \"Introduction to Social Media Scraping\"delay=.6s >}}'\nsubtitle: '{{< animate fadeInDown \"Learn the basic tools for scraping Twitter Data in R\"delay=.6s >}}'\nauthor: [\"Lukas Lehmann\"]\ndate: \"2023-04-18\"\ncategories: [\"Twitter\", \"Webscraping\", \"Text\"]\ntoc: true\ndraft: false\ncode-link: true\ncode-copy: true\ntitle-block-banner: true\ncomments: false\nimage: images/Social.png\ninclude-in-header: meta.html\nfilters:\n   - lightbox\nlightbox: \n  match: auto\n  effect: fade\n  desc-position: left\n  css-class: \"lightwidth\"\n---\n\n\n::: {.callout-important}\n## NEWS\n\nWith the next Twitter regulations, The free package comes with rate-limited access to v2 tweet posting and media upload endpoints, with a posting limit of 1,500 tweets per month at the app level. If you want to access to posting tweets are up to 3,000 per month, and for reading, it's up to 10,000 tweets per month, you will need to pay 100 USD per month. However, you can sill acces to databases and use this tutorial to analyze text in general. \n:::\n\n\n\n# Introduction \n\n------------------------------------------------------------------------\n\n\nSocial media has become an important source of information and communication for individuals, businesses, and organizations; actually there is a growing need to analyze social media data to gain insights into user behavior, sentiment, and trends. With the NLP trend and other inputs,  Social media scraping, which involves collecting and analyzing data from social media sites, such as Twitter, has emerged as a powerful tool for such analysis.\n\nIn this introduction, we will explore the basics of social media scraping using R and Twitter, including how to access the Twitter API, retrieve data from Twitter, and perform basic analyses on the data. We will also discuss some of the ethical and legal considerations of social media scraping, and provide tips for using social media scraping responsibly.\n\n\n¬†\n\n## Loading in packages and authorizing rtweet\n\n\n\nSo the first thing we want to do is load in the packages we'll be using to scrape and manipulate our data. The most important of those is rtweet, which is the one we'll be using to interact with the Twitter API.\n\nIn order to scrape tweets, you'll need a Twitter developer account and have to make a Twitter app. This is actually a pretty simple process (and won't require any coding). [Here's a step-by-step guide](https://jtr13.github.io/cc21fall2/scrape-twitter-data-using-r.html) \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(rtweet, tidyverse, ggplot2, utils, tm, SnowballC, caTools, \n               rpart, topicmodels, tidytext, wordcloud, lexicon, reshape2,\n               sentimentr)\n```\n:::\n\n\nRunning the code above (without the #'s) will prompt a dialogue box to pop up on your screen asking you for a bearer token. You can find that on the Twitter developer page. I made the last two lines into comments so that this can be knit into HTML smoothly.\n\n\n\n# Scraping Tweets\n\n------------------------------------------------------------------------\n\n\nWe'll be make two datasets: one containing tweets just from US President Biden's Twitter account and the other scraping the most recent (English language) tweets from all Twitter accounts mentioning the word \"green.\" \n\n\n::: {.cell}\n\n```{.r .cell-code}\nbiden_tweets <- read_csv(\"https://raw.githubusercontent.com/lukaslehmann-R/common_files/main/biden_tweets.csv\")\ngreen <- read_csv(\"https://raw.githubusercontent.com/lukaslehmann-R/common_files/main/green.csv\")\n```\n:::\n\n\n¬†\n\nHere are the Biden tweets. We show the first 6 entries of the `biden_tweets` database\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"created_at\"],\"name\":[1],\"type\":[\"dttm\"],\"align\":[\"right\"]},{\"label\":[\"id\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"id_str\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"text\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"full_text\"],\"name\":[5],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"truncated\"],\"name\":[6],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"entities\"],\"name\":[7],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"source\"],\"name\":[8],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"in_reply_to_status_id\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"in_reply_to_status_id_str\"],\"name\":[10],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"in_reply_to_user_id\"],\"name\":[11],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"in_reply_to_user_id_str\"],\"name\":[12],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"in_reply_to_screen_name\"],\"name\":[13],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"geo\"],\"name\":[14],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"coordinates\"],\"name\":[15],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"place\"],\"name\":[16],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"contributors\"],\"name\":[17],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"is_quote_status\"],\"name\":[18],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"retweet_count\"],\"name\":[19],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"favorite_count\"],\"name\":[20],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"favorited\"],\"name\":[21],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"favorited_by\"],\"name\":[22],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"retweeted\"],\"name\":[23],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"scopes\"],\"name\":[24],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"lang\"],\"name\":[25],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"possibly_sensitive\"],\"name\":[26],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"display_text_width\"],\"name\":[27],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"display_text_range\"],\"name\":[28],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"retweeted_status\"],\"name\":[29],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"quoted_status\"],\"name\":[30],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"quoted_status_id\"],\"name\":[31],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"quoted_status_id_str\"],\"name\":[32],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"quoted_status_permalink\"],\"name\":[33],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"quote_count\"],\"name\":[34],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"timestamp_ms\"],\"name\":[35],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"reply_count\"],\"name\":[36],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"filter_level\"],\"name\":[37],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"metadata\"],\"name\":[38],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"query\"],\"name\":[39],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"withheld_scope\"],\"name\":[40],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"withheld_copyright\"],\"name\":[41],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"withheld_in_countries\"],\"name\":[42],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"possibly_sensitive_appealable\"],\"name\":[43],\"type\":[\"lgl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"2023-03-17 03:14:37\",\"2\":\"1.636567e+18\",\"3\":\"1.636567e+18\",\"4\":\"Sanofi is now the latest manufacturer to cap the cost of insulin for everyone.\\\\n¬†\\\\nThat means three of America's largest insulin producers agreed to reduce their prices following my call to expand my $35 monthly cap for seniors to all Americans.\\\\n\\\\nCongress, let's make it law. https://t.co/AVtEQUtwr0\",\"5\":\"Sanofi is now the latest manufacturer to cap the cost of insulin for everyone.\\\\n¬†\\\\nThat means three of America's largest insulin producers agreed to reduce their prices following my call to expand my $35 monthly cap for seniors to all Americans.\\\\n\\\\nCongress, let's make it law. https://t.co/AVtEQUtwr0\",\"6\":\"FALSE\",\"7\":\"NA\",\"8\":\"<a href=\\\"http://twitter.com/download/iphone\\\" rel=\\\"nofollow\\\">Twitter for iPhone<\\/a>\",\"9\":\"NA\",\"10\":\"NA\",\"11\":\"NA\",\"12\":\"NA\",\"13\":\"NA\",\"14\":\"NA\",\"15\":\"NA\",\"16\":\"NA\",\"17\":\"NA\",\"18\":\"TRUE\",\"19\":\"1721\",\"20\":\"8696\",\"21\":\"FALSE\",\"22\":\"NA\",\"23\":\"FALSE\",\"24\":\"NA\",\"25\":\"en\",\"26\":\"FALSE\",\"27\":\"NA\",\"28\":\"273\",\"29\":\"NA\",\"30\":\"NA\",\"31\":\"1.636495e+18\",\"32\":\"1.636495e+18\",\"33\":\"NA\",\"34\":\"NA\",\"35\":\"NA\",\"36\":\"NA\",\"37\":\"NA\",\"38\":\"NA\",\"39\":\"NA\",\"40\":\"NA\",\"41\":\"NA\",\"42\":\"NA\",\"43\":\"NA\"},{\"1\":\"2023-03-17 01:00:49\",\"2\":\"1.636533e+18\",\"3\":\"1.636533e+18\",\"4\":\"If MAGA Republicans in Congress get their way, as many as one hundred million hardworking Americans could lose protections to their health care. https://t.co/lwbQSFwqWv\",\"5\":\"If MAGA Republicans in Congress get their way, as many as one hundred million hardworking Americans could lose protections to their health care. https://t.co/lwbQSFwqWv\",\"6\":\"FALSE\",\"7\":\"NA\",\"8\":\"<a href=\\\"https://www.sprinklr.com\\\" rel=\\\"nofollow\\\">The White House<\\/a>\",\"9\":\"NA\",\"10\":\"NA\",\"11\":\"NA\",\"12\":\"NA\",\"13\":\"NA\",\"14\":\"NA\",\"15\":\"NA\",\"16\":\"NA\",\"17\":\"NA\",\"18\":\"FALSE\",\"19\":\"1394\",\"20\":\"4420\",\"21\":\"FALSE\",\"22\":\"NA\",\"23\":\"FALSE\",\"24\":\"NA\",\"25\":\"en\",\"26\":\"FALSE\",\"27\":\"NA\",\"28\":\"144\",\"29\":\"NA\",\"30\":\"NA\",\"31\":\"NA\",\"32\":\"NA\",\"33\":\"NA\",\"34\":\"NA\",\"35\":\"NA\",\"36\":\"NA\",\"37\":\"NA\",\"38\":\"NA\",\"39\":\"NA\",\"40\":\"NA\",\"41\":\"NA\",\"42\":\"NA\",\"43\":\"NA\"},{\"1\":\"2023-03-16 21:47:02\",\"2\":\"1.636484e+18\",\"3\":\"1.636484e+18\",\"4\":\"We lowered health care costs and made it easier to sign up for the Affordable Care Act.\\\\n \\\\nNow, more Americans have health insurance than ever.\",\"5\":\"We lowered health care costs and made it easier to sign up for the Affordable Care Act.\\\\n \\\\nNow, more Americans have health insurance than ever.\",\"6\":\"FALSE\",\"7\":\"NA\",\"8\":\"<a href=\\\"https://www.sprinklr.com\\\" rel=\\\"nofollow\\\">The White House<\\/a>\",\"9\":\"NA\",\"10\":\"NA\",\"11\":\"NA\",\"12\":\"NA\",\"13\":\"NA\",\"14\":\"NA\",\"15\":\"NA\",\"16\":\"NA\",\"17\":\"NA\",\"18\":\"FALSE\",\"19\":\"935\",\"20\":\"4270\",\"21\":\"FALSE\",\"22\":\"NA\",\"23\":\"FALSE\",\"24\":\"NA\",\"25\":\"en\",\"26\":\"NA\",\"27\":\"NA\",\"28\":\"142\",\"29\":\"NA\",\"30\":\"NA\",\"31\":\"NA\",\"32\":\"NA\",\"33\":\"NA\",\"34\":\"NA\",\"35\":\"NA\",\"36\":\"NA\",\"37\":\"NA\",\"38\":\"NA\",\"39\":\"NA\",\"40\":\"NA\",\"41\":\"NA\",\"42\":\"NA\",\"43\":\"NA\"},{\"1\":\"2023-03-16 19:45:00\",\"2\":\"1.636454e+18\",\"3\":\"1.636454e+18\",\"4\":\"For two years, our Administration has led the way on deploying cost-cutting clean energy investments ‚Äì from tax credits for rooftop solar and electric vehicles to environmental justice solutions to ease burdens on families.\\\\n\\\\nMy budget will build on that progress.\",\"5\":\"For two years, our Administration has led the way on deploying cost-cutting clean energy investments ‚Äì from tax credits for rooftop solar and electric vehicles to environmental justice solutions to ease burdens on families.\\\\n\\\\nMy budget will build on that progress.\",\"6\":\"FALSE\",\"7\":\"NA\",\"8\":\"<a href=\\\"https://www.sprinklr.com\\\" rel=\\\"nofollow\\\">The White House<\\/a>\",\"9\":\"NA\",\"10\":\"NA\",\"11\":\"NA\",\"12\":\"NA\",\"13\":\"NA\",\"14\":\"NA\",\"15\":\"NA\",\"16\":\"NA\",\"17\":\"NA\",\"18\":\"FALSE\",\"19\":\"704\",\"20\":\"3172\",\"21\":\"FALSE\",\"22\":\"NA\",\"23\":\"FALSE\",\"24\":\"NA\",\"25\":\"en\",\"26\":\"NA\",\"27\":\"NA\",\"28\":\"263\",\"29\":\"NA\",\"30\":\"NA\",\"31\":\"NA\",\"32\":\"NA\",\"33\":\"NA\",\"34\":\"NA\",\"35\":\"NA\",\"36\":\"NA\",\"37\":\"NA\",\"38\":\"NA\",\"39\":\"NA\",\"40\":\"NA\",\"41\":\"NA\",\"42\":\"NA\",\"43\":\"NA\"},{\"1\":\"2023-03-16 19:22:47\",\"2\":\"1.636448e+18\",\"3\":\"1.636448e+18\",\"4\":\"No working-class American should pay more in taxes than a billionaire.\\\\n\\\\nTake it from a few Philly union workers: https://t.co/4HecdLWzpN\",\"5\":\"No working-class American should pay more in taxes than a billionaire.\\\\n\\\\nTake it from a few Philly union workers: https://t.co/4HecdLWzpN\",\"6\":\"FALSE\",\"7\":\"NA\",\"8\":\"<a href=\\\"https://www.sprinklr.com\\\" rel=\\\"nofollow\\\">The White House<\\/a>\",\"9\":\"NA\",\"10\":\"NA\",\"11\":\"NA\",\"12\":\"NA\",\"13\":\"NA\",\"14\":\"NA\",\"15\":\"NA\",\"16\":\"NA\",\"17\":\"NA\",\"18\":\"FALSE\",\"19\":\"1587\",\"20\":\"7263\",\"21\":\"FALSE\",\"22\":\"NA\",\"23\":\"FALSE\",\"24\":\"NA\",\"25\":\"en\",\"26\":\"FALSE\",\"27\":\"NA\",\"28\":\"112\",\"29\":\"NA\",\"30\":\"NA\",\"31\":\"NA\",\"32\":\"NA\",\"33\":\"NA\",\"34\":\"NA\",\"35\":\"NA\",\"36\":\"NA\",\"37\":\"NA\",\"38\":\"NA\",\"39\":\"NA\",\"40\":\"NA\",\"41\":\"NA\",\"42\":\"NA\",\"43\":\"NA\"},{\"1\":\"2023-03-16 16:34:03\",\"2\":\"1.636405e+18\",\"3\":\"1.636405e+18\",\"4\":\"Good luck to every team in this year's NCAA tournament. \\\\n\\\\nI‚Äôve got Arizona in the men‚Äôs tournament and Villanova in the women‚Äôs ‚Äì and as you know, in this household, Villanova always wins. https://t.co/iILzABTCuo\",\"5\":\"Good luck to every team in this year's NCAA tournament. \\\\n\\\\nI‚Äôve got Arizona in the men‚Äôs tournament and Villanova in the women‚Äôs ‚Äì and as you know, in this household, Villanova always wins. https://t.co/iILzABTCuo\",\"6\":\"FALSE\",\"7\":\"NA\",\"8\":\"<a href=\\\"https://www.sprinklr.com\\\" rel=\\\"nofollow\\\">The White House<\\/a>\",\"9\":\"NA\",\"10\":\"NA\",\"11\":\"NA\",\"12\":\"NA\",\"13\":\"NA\",\"14\":\"NA\",\"15\":\"NA\",\"16\":\"NA\",\"17\":\"NA\",\"18\":\"FALSE\",\"19\":\"1525\",\"20\":\"15398\",\"21\":\"FALSE\",\"22\":\"NA\",\"23\":\"FALSE\",\"24\":\"NA\",\"25\":\"en\",\"26\":\"FALSE\",\"27\":\"NA\",\"28\":\"188\",\"29\":\"NA\",\"30\":\"NA\",\"31\":\"NA\",\"32\":\"NA\",\"33\":\"NA\",\"34\":\"NA\",\"35\":\"NA\",\"36\":\"NA\",\"37\":\"NA\",\"38\":\"NA\",\"39\":\"NA\",\"40\":\"NA\",\"41\":\"NA\",\"42\":\"NA\",\"43\":\"NA\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n¬†\n\nAnd here are the databases with words related to green. The `green` database.\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"created_at\"],\"name\":[1],\"type\":[\"dttm\"],\"align\":[\"right\"]},{\"label\":[\"id\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"id_str\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"text\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"full_text\"],\"name\":[5],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"truncated\"],\"name\":[6],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"entities\"],\"name\":[7],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"source\"],\"name\":[8],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"in_reply_to_status_id\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"in_reply_to_status_id_str\"],\"name\":[10],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"in_reply_to_user_id\"],\"name\":[11],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"in_reply_to_user_id_str\"],\"name\":[12],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"in_reply_to_screen_name\"],\"name\":[13],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"geo\"],\"name\":[14],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"coordinates\"],\"name\":[15],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"place\"],\"name\":[16],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"contributors\"],\"name\":[17],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"is_quote_status\"],\"name\":[18],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"retweet_count\"],\"name\":[19],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"favorite_count\"],\"name\":[20],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"favorited\"],\"name\":[21],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"favorited_by\"],\"name\":[22],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"retweeted\"],\"name\":[23],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"scopes\"],\"name\":[24],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"lang\"],\"name\":[25],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"possibly_sensitive\"],\"name\":[26],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"display_text_width\"],\"name\":[27],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"display_text_range\"],\"name\":[28],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"retweeted_status\"],\"name\":[29],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"quoted_status\"],\"name\":[30],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"quoted_status_id\"],\"name\":[31],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"quoted_status_id_str\"],\"name\":[32],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"quoted_status_permalink\"],\"name\":[33],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"quote_count\"],\"name\":[34],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"timestamp_ms\"],\"name\":[35],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"reply_count\"],\"name\":[36],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"filter_level\"],\"name\":[37],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"metadata\"],\"name\":[38],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"query\"],\"name\":[39],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"withheld_scope\"],\"name\":[40],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"withheld_copyright\"],\"name\":[41],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"withheld_in_countries\"],\"name\":[42],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"possibly_sensitive_appealable\"],\"name\":[43],\"type\":[\"lgl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"2023-03-15 14:16:36\",\"2\":\"1.636008e+18\",\"3\":\"1.636008e+18\",\"4\":\"Plymouth City Council looked at this green space in their City &amp; thought 'You know what? More concrete here will be awesome!' \\\\n Apparently the Council went ahead &amp; destroyed 100 mature trees last night, I'm disgusted!üò°üò°\\\\nLet's make them famous..\\\\n#PlymouthCityCouncilTreeMassacre https://t.co/HZnhbaw2qB\",\"5\":\"Plymouth City Council looked at this green space in their City &amp; thought 'You know what? More concrete here will be awesome!' \\\\n Apparently the Council went ahead &amp; destroyed 100 mature trees last night, I'm disgusted!üò°üò°\\\\nLet's make them famous..\\\\n#PlymouthCityCouncilTreeMassacre https://t.co/HZnhbaw2qB\",\"6\":\"FALSE\",\"7\":\"NA\",\"8\":\"<a href=\\\"https://mobile.twitter.com\\\" rel=\\\"nofollow\\\">Twitter Web App<\\/a>\",\"9\":\"NA\",\"10\":\"NA\",\"11\":\"NA\",\"12\":\"NA\",\"13\":\"NA\",\"14\":\"NA\",\"15\":\"NA\",\"16\":\"NA\",\"17\":\"NA\",\"18\":\"TRUE\",\"19\":\"1793\",\"20\":\"2962\",\"21\":\"FALSE\",\"22\":\"NA\",\"23\":\"FALSE\",\"24\":\"NA\",\"25\":\"en\",\"26\":\"FALSE\",\"27\":\"NA\",\"28\":\"285\",\"29\":\"NA\",\"30\":\"NA\",\"31\":\"1.634279e+18\",\"32\":\"1.634279e+18\",\"33\":\"NA\",\"34\":\"NA\",\"35\":\"NA\",\"36\":\"NA\",\"37\":\"NA\",\"38\":\"NA\",\"39\":\"NA\",\"40\":\"NA\",\"41\":\"NA\",\"42\":\"NA\",\"43\":\"NA\"},{\"1\":\"2023-03-15 22:27:10\",\"2\":\"1.636132e+18\",\"3\":\"1.636132e+18\",\"4\":\"üá≥üá± Really bad news. The first election results are in: The ruling parties suffered some losses, but with the help of the Labour Party and the green-left they will still be able to form a majority in the Senate to pass the nitrogen policies, including expropriation. #DutchFarmers\",\"5\":\"üá≥üá± Really bad news. The first election results are in: The ruling parties suffered some losses, but with the help of the Labour Party and the green-left they will still be able to form a majority in the Senate to pass the nitrogen policies, including expropriation. #DutchFarmers\",\"6\":\"FALSE\",\"7\":\"NA\",\"8\":\"<a href=\\\"http://twitter.com/download/iphone\\\" rel=\\\"nofollow\\\">Twitter for iPhone<\\/a>\",\"9\":\"NA\",\"10\":\"NA\",\"11\":\"NA\",\"12\":\"NA\",\"13\":\"NA\",\"14\":\"NA\",\"15\":\"NA\",\"16\":\"NA\",\"17\":\"NA\",\"18\":\"FALSE\",\"19\":\"3155\",\"20\":\"11096\",\"21\":\"FALSE\",\"22\":\"NA\",\"23\":\"FALSE\",\"24\":\"NA\",\"25\":\"en\",\"26\":\"NA\",\"27\":\"NA\",\"28\":\"279\",\"29\":\"NA\",\"30\":\"NA\",\"31\":\"NA\",\"32\":\"NA\",\"33\":\"NA\",\"34\":\"NA\",\"35\":\"NA\",\"36\":\"NA\",\"37\":\"NA\",\"38\":\"NA\",\"39\":\"NA\",\"40\":\"NA\",\"41\":\"NA\",\"42\":\"NA\",\"43\":\"NA\"},{\"1\":\"2023-03-16 11:10:44\",\"2\":\"1.636324e+18\",\"3\":\"1.636324e+18\",\"4\":\"Compare &amp; contrast.\\\\n  Top is Plymouth City centre in all its green glory.\\\\n Bottom is Plymouth City centre today, after @plymouthcc snuck out in the night to destroy over 100 trees... it looks like a scene from #TheLastOfUs üò± \\\\nWho's angry? ü§¨\\\\n#PlymouthCityCouncilTreeMassacre https://t.co/hTv2baRFVE\",\"5\":\"Compare &amp; contrast.\\\\n  Top is Plymouth City centre in all its green glory.\\\\n Bottom is Plymouth City centre today, after @plymouthcc snuck out in the night to destroy over 100 trees... it looks like a scene from #TheLastOfUs üò± \\\\nWho's angry? ü§¨\\\\n#PlymouthCityCouncilTreeMassacre https://t.co/hTv2baRFVE\",\"6\":\"FALSE\",\"7\":\"NA\",\"8\":\"<a href=\\\"http://twitter.com/download/android\\\" rel=\\\"nofollow\\\">Twitter for Android<\\/a>\",\"9\":\"NA\",\"10\":\"NA\",\"11\":\"NA\",\"12\":\"NA\",\"13\":\"NA\",\"14\":\"NA\",\"15\":\"NA\",\"16\":\"NA\",\"17\":\"NA\",\"18\":\"FALSE\",\"19\":\"1288\",\"20\":\"3515\",\"21\":\"FALSE\",\"22\":\"NA\",\"23\":\"FALSE\",\"24\":\"NA\",\"25\":\"en\",\"26\":\"FALSE\",\"27\":\"NA\",\"28\":\"277\",\"29\":\"NA\",\"30\":\"NA\",\"31\":\"NA\",\"32\":\"NA\",\"33\":\"NA\",\"34\":\"NA\",\"35\":\"NA\",\"36\":\"NA\",\"37\":\"NA\",\"38\":\"NA\",\"39\":\"NA\",\"40\":\"NA\",\"41\":\"NA\",\"42\":\"NA\",\"43\":\"NA\"},{\"1\":\"2023-03-17 01:09:24\",\"2\":\"1.636535e+18\",\"3\":\"1.636535e+18\",\"4\":\"RT @420: keep it green https://t.co/zipq7GjUjs\",\"5\":\"RT @420: keep it green https://t.co/zipq7GjUjs\",\"6\":\"FALSE\",\"7\":\"NA\",\"8\":\"<a href=\\\"http://twitter.com/download/android\\\" rel=\\\"nofollow\\\">Twitter for Android<\\/a>\",\"9\":\"NA\",\"10\":\"NA\",\"11\":\"NA\",\"12\":\"NA\",\"13\":\"NA\",\"14\":\"NA\",\"15\":\"NA\",\"16\":\"NA\",\"17\":\"NA\",\"18\":\"FALSE\",\"19\":\"881\",\"20\":\"0\",\"21\":\"FALSE\",\"22\":\"NA\",\"23\":\"FALSE\",\"24\":\"NA\",\"25\":\"en\",\"26\":\"FALSE\",\"27\":\"NA\",\"28\":\"46\",\"29\":\"NA\",\"30\":\"NA\",\"31\":\"NA\",\"32\":\"NA\",\"33\":\"NA\",\"34\":\"NA\",\"35\":\"NA\",\"36\":\"NA\",\"37\":\"NA\",\"38\":\"NA\",\"39\":\"NA\",\"40\":\"NA\",\"41\":\"NA\",\"42\":\"NA\",\"43\":\"NA\"},{\"1\":\"2023-03-17 01:09:23\",\"2\":\"1.636535e+18\",\"3\":\"1.636535e+18\",\"4\":\"Green mountains are always accompanied by green waters  haemcupature .1231 \\\\n#Á∫πË∫´ #ËÉ∏Êé® #ÈÅìÂÖ∑Â§ßËµõ https://t.co/KZJti0SlTr\",\"5\":\"Green mountains are always accompanied by green waters  haemcupature .1231 \\\\n#Á∫πË∫´ #ËÉ∏Êé® #ÈÅìÂÖ∑Â§ßËµõ https://t.co/KZJti0SlTr\",\"6\":\"FALSE\",\"7\":\"NA\",\"8\":\"<a href=\\\"https://mobile.twitter.com\\\" rel=\\\"nofollow\\\">Twitter Web App<\\/a>\",\"9\":\"NA\",\"10\":\"NA\",\"11\":\"NA\",\"12\":\"NA\",\"13\":\"NA\",\"14\":\"NA\",\"15\":\"NA\",\"16\":\"NA\",\"17\":\"NA\",\"18\":\"FALSE\",\"19\":\"0\",\"20\":\"0\",\"21\":\"FALSE\",\"22\":\"NA\",\"23\":\"FALSE\",\"24\":\"NA\",\"25\":\"en\",\"26\":\"FALSE\",\"27\":\"NA\",\"28\":\"89\",\"29\":\"NA\",\"30\":\"NA\",\"31\":\"NA\",\"32\":\"NA\",\"33\":\"NA\",\"34\":\"NA\",\"35\":\"NA\",\"36\":\"NA\",\"37\":\"NA\",\"38\":\"NA\",\"39\":\"NA\",\"40\":\"NA\",\"41\":\"NA\",\"42\":\"NA\",\"43\":\"NA\"},{\"1\":\"2023-03-17 01:09:22\",\"2\":\"1.636535e+18\",\"3\":\"1.636535e+18\",\"4\":\"Green $50 a line tap in https://t.co/PNiTEFxglV\",\"5\":\"Green $50 a line tap in https://t.co/PNiTEFxglV\",\"6\":\"FALSE\",\"7\":\"NA\",\"8\":\"<a href=\\\"http://twitter.com/download/iphone\\\" rel=\\\"nofollow\\\">Twitter for iPhone<\\/a>\",\"9\":\"NA\",\"10\":\"NA\",\"11\":\"NA\",\"12\":\"NA\",\"13\":\"NA\",\"14\":\"NA\",\"15\":\"NA\",\"16\":\"NA\",\"17\":\"NA\",\"18\":\"FALSE\",\"19\":\"0\",\"20\":\"0\",\"21\":\"FALSE\",\"22\":\"NA\",\"23\":\"FALSE\",\"24\":\"NA\",\"25\":\"en\",\"26\":\"FALSE\",\"27\":\"NA\",\"28\":\"23\",\"29\":\"NA\",\"30\":\"NA\",\"31\":\"NA\",\"32\":\"NA\",\"33\":\"NA\",\"34\":\"NA\",\"35\":\"NA\",\"36\":\"NA\",\"37\":\"NA\",\"38\":\"NA\",\"39\":\"NA\",\"40\":\"NA\",\"41\":\"NA\",\"42\":\"NA\",\"43\":\"NA\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nI turned the first two lines into comments for the same reason as earlier. The third and fourth lines will load in the same datasets (although the data within will different because Biden and other Twitter users will continue to tweet)\n\n\n¬†\n\n# Frequency of Biden tweets\n\n------------------------------------------------------------------------\n\n\nNow that we have our tweets, let's construct a time series plot that shows the frequency of tweets by President Joe Biden over time. The data is aggregated using two-week intervals. The plot displays the number of tweets on the y-axis and time on the x-axis. Each point on the plot represents the number of tweets for a particular two-week interval.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Create time series plot using ts_plot() function from tsbox package\n# \"2 weeks\" specifies the interval for aggregating the data\nbiden_tweets %>% ts_plot(\"2 weeks\") +\n# Customize plot appearance using ggplot2 functions\nggplot2::theme_minimal(base_size=16) +  # remove background gridlines and borders\nggplot2::theme(plot.title = ggplot2::element_text(face = \"bold\")) +  # set plot title to boldface font\nggplot2::labs(x = NULL, y = \"Number of Tweets\",  # set labels for x and y axis\n     title = \"Frequency of Biden tweets\",  # set plot title\n     subtitle = \"Tweet counts aggregated using two-week intervals\",  # set plot subtitle\n     caption = \"\\nSource: Data collected from Twitter's REST API via rtweet\") +  # set plot caption\ngeom_point(size=6) + # add points to plot to show tweet count at each interval\ngeom_line(linewidth=1.5)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/first_plot-1.png){width=80%}\n:::\n:::\n\n\n\n¬†\n\n# Topic Modeling \n\n------------------------------------------------------------------------\n\nThere's a lot that goes into explaining what a topic model is. One kind of topic model is called LDA, and you can read all about it [here](https://www.tidytextmining.com/topicmodeling.html)\n\n¬†\n\n## Corpus\n\nIn natural language processing (NLP), a corpus is a collection of written or spoken language that is used as a basis for analysis. A corpus can be made up of many different types of texts, including books, articles, speeches, social media posts, and more. Here, we will make a corpus of documents using just the full_text part of green tweets\n\n::: {.cell}\n\n```{.r .cell-code}\ncorpus1 <- Corpus(VectorSource(green$full_text))\n```\n:::\n\n\n\nNow we need to clean our text a bit. Change to lower case and remove punctuation!\n\n::: {.cell}\n\n```{.r .cell-code}\ncorpus1 <- tm_map(corpus1, tolower)\ncorpus1 <- tm_map(corpus1, removePunctuation)\n#We need to remove stop words to get meaningful results from this exercise. \n#We'll remove words like \"me\", \"is\", \"was\"\nstopwords(\"english\")[1:50]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"i\"          \"me\"         \"my\"         \"myself\"     \"we\"        \n [6] \"our\"        \"ours\"       \"ourselves\"  \"you\"        \"your\"      \n[11] \"yours\"      \"yourself\"   \"yourselves\" \"he\"         \"him\"       \n[16] \"his\"        \"himself\"    \"she\"        \"her\"        \"hers\"      \n[21] \"herself\"    \"it\"         \"its\"        \"itself\"     \"they\"      \n[26] \"them\"       \"their\"      \"theirs\"     \"themselves\" \"what\"      \n[31] \"which\"      \"who\"        \"whom\"       \"this\"       \"that\"      \n[36] \"these\"      \"those\"      \"am\"         \"is\"         \"are\"       \n[41] \"was\"        \"were\"       \"be\"         \"been\"       \"being\"     \n[46] \"have\"       \"has\"        \"had\"        \"having\"     \"do\"        \n```\n:::\n\n```{.r .cell-code}\ncorpus1 <- tm_map(corpus1, removeWords, (stopwords(\"english\")))\n#We need to clean the words in the corpus further by \"stemming\" words\n#A word like \"understand\" and \"understands\" will both become \"understand\"\ncorpus1 <- tm_map(corpus1, stemDocument)\n```\n:::\n\n\n¬†\n\n## Document Term Matrix\n\nSo, a DTM is a way of representing a collection of text documents quantitativelythat allows us to do some cool stuff with them. It's basically a matrix where the rows correspond to the documents in the collection, and the columns correspond to the unique words or terms that appear in the documents. Each cell in the matrix represents the frequency of a particular term in a particular document.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#creates a document term matrix, which is necessary for building a topic model\nDTM1 <- DocumentTermMatrix(corpus1)\n#Here we can see the most frequently used terms\nfrequent_ge_20 <- findFreqTerms(DTM1, lowfreq = 100)\nfrequent_ge_20\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"amp\"       \"green\"     \"like\"      \"store\"     \"day\"       \"‚Ä¶\"        \n [7] \"‚Äôs\"        \"light\"     \"one\"       \"year\"      \"figur\"     \"red\"      \n[13] \"buddha\"    \"fasc1nat\"  \"four\"      \"f‚Ä¶\"        \"philippin\" \"pray\"     \n[19] \"purchas\"   \"spent\"     \"woman\"    \n```\n:::\n:::\n\n\n¬†\n\n## Let's create the topic model! We'll start with 5 topics\n\nThe code snippet performs topic modeling on a corpus of text data represented as a Document-Term Matrix (DTM) using the Latent Dirichlet Allocation (LDA) algorithm. The LDA() function from the topic models package is used to create a model with 7 topics and a specified random seed for reproducibility. The resulting model is stored in the green_lda1 object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Perform LDA topic modeling on a Document-Term Matrix (DTM) with 7 topics\ngreen_lda1 <- LDA(DTM1, k = 7, control = list(seed = 1234))\n\n#Print the model summary\ngreen_lda1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nA LDA_VEM topic model with 7 topics.\n```\n:::\n\n```{.r .cell-code}\n#Convert the model's beta matrix to a tidy format\ngreen_topics1 <- tidy(green_lda1, matrix = \"beta\")\n\ngreen_top_terms1 <- green_topics1 %>%\n  group_by(topic) %>% #Group the terms by topic\n  slice_max(beta, n = 10) %>% #Top 10 terms with the highest probabilities\n  ungroup() %>% #Remove the grouping attribute from the data frame\n  arrange(topic, -beta) #Sort the data frame by topic index and term probability\n```\n:::\n\n\n¬†\n\n# Top 10 Terms by Topic\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggplot2)\nlibrary(tidytext)\nlibrary(ggtext)\n\n#Reorder the terms within each topic based on their probability (beta) values\ngreen_top_terms1 %>%\n  mutate(term = reorder_within(term, beta, topic)) %>%\n#Create a bar plot of the term probabilities for each topic\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE, width = 0.8) +\n  scale_fill_manual(values = c(\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#9467bd\", \"#8c564b\", \"#e377c2\")) +\n  theme_minimal()+\n#Create separate plots for each topic and adjust the y-axis limits for each plot\n  facet_wrap(~ topic, scales = \"free\", ncol = 2, strip.position = \"bottom\") +\n  theme(strip.background = element_blank(),\n        strip.text = element_text(size = 12, face = \"bold\")) +\n#Apply a custom scale for the y-axis that preserves the within-topic ordering of terms\n  scale_y_reordered(expand = c(0, 0)) +\n  labs(title = \"Top 10 Terms by Topic\",\n       x = \"Term Probability\",\n       y = NULL,\n       caption = \"Source: LDA Topic Model of Green Energy Tweets\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/second_plot-1.png){width=80%}\n:::\n:::\n\n\n\n\nThe plot is a bar chart showing the top 10 terms for each of the 7 topics generated by an LDA topic modeling algorithm. Each topic is represented by a different color, and the bars are sorted in descending order of probability (beta) within each topic. The x-axis shows the probability (beta) of each term being associated with the topic, ranging from 0 at the left side of the plot to 0.05 at the right side of the plot. The y-axis shows the top 10 terms for each topic, with each topic in a separate facet of the plot.\n\nOverall, the plot allows us to see which terms are most strongly associated with each of the 7 topics. For example, we can see that for topic 1 (represented by blue bars), the most strongly associated terms are \"climate\", \"change\", \"renewable\", and \"energy\", while for topic 5 (represented by purple bars), the most strongly associated terms are \"carbon\", \"footprint\", and \"reduction\".\n\n¬†\n\n# Word Cloud\n\n------------------------------------------------------------------------\n\nNow let's go back to the Biden tweets and make a word cloud. What's a word cloud? A word cloud is a graphical representation of textual data in which the size of each word represents its frequency or importance. It's a popular visualization tool in data analysis and is often used to quickly identify the most common words in a text or group of texts. You can read about it in detail  [here](https://towardsdatascience.com/create-a-word-cloud-with-r-bde3e7422e8a)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwords_data <- biden_tweets %>% \n  select(text)%>%\n  unnest_tokens(word, text) \n#Let's get rid of some words associated with links and things that cause errors\nwords_data <- words_data %>% \n  filter(!word %in% c('https', 't.co', 'he\\'s', 'i\\'m', 'it\\'s'))\n#Let's get rid of stopwords\nwords_data2 <- words_data %>%\n  anti_join(stop_words) %>%\n  count(word, sort = TRUE)\n```\n:::\n\n\n¬†\n\n\n## Word Cloud of Biden Tweets\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# Create custom color palette\nmy_palette <- brewer.pal(8, \"Dark2\")\n\n# Create word cloud with larger font size and custom layout\nwordcloud(words_data2$word, \n          words_data2$n, \n          max.words = 200, \n          colors = my_palette, \n          scale = c(5, 0.3),\n          random.order = FALSE,\n          rot.per = 0.25,\n          random.color = TRUE,\n          main = \"Word Cloud of Biden Tweets\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/third_plot-1.png){width=80%}\n:::\n:::\n\n\n\n¬†\n\n## Comparison Cloud of Biden Tweets by Sentiment\n\nNow let's make a word cloud from those tweets but highlight which words are positive and which are negative \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Select and tokenize words\nwords_data <- biden_tweets %>% \n  select(text) %>%\n  unnest_tokens(word, text) \n\n#Get sentiment scores for each word using the bing lexicon\nsentiment_scores <- words_data2 %>%\n  inner_join(get_sentiments(\"bing\"))\n\n#Count the number of words in each sentiment category\nsentiment_counts <- sentiment_scores %>%\n  count(sentiment, sort = TRUE)\n\n#Create a list of profanity words to remove from the dataset\nprofanity_list <- unique(tolower(lexicon::profanity_alvarez))\n\n#Filter out stop words and profanity words from the dataset\nfiltered_words_data <- words_data %>%\n  filter(!word %in% c('https', 't.co', 'he\\'s', 'i\\'m', 'it\\'s', profanity_list))\n\n#Get sentiment scores for each filtered word using the bing lexicon\nfiltered_sentiment_scores <- filtered_words_data %>%\n  inner_join(get_sentiments(\"bing\"))\n```\n:::\n\n\n\n¬†\n\n## Cloud\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n#Count the number of filtered words with each sentiment score\nword_sentiment_counts <- filtered_sentiment_scores %>%\n  count(word, sentiment, sort = TRUE) %>%\n  acast(word ~ sentiment, value.var = \"n\", fill = 0)\n\n#Create a comparison cloud using the word and sentiment counts\ncomparison.cloud(word_sentiment_counts, \n                  colors = c(\"red\", \"blue\"),\n                  max.words = Inf)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/fourth_plot-1.png){width=80%}\n:::\n:::\n\n\n\n\n¬†\n\n\n# Sentiment Analysis\n\nNow we are going to see how many of President Biden's tweets can be classified as positive, neutral, and negative. `sentimentr` is the main package in use here. The code snippet provided takes the tweets from Biden's Twitter account and extracts individual sentences from each tweet. It then calculates the sentiment score for each sentence using the `sentiment()`, which uses a lexicon of positive and negative words to score each sentence as positive or negative. The sentiment scores for each sentence are then aggregated to obtain the mean sentiment score for each tweet. Based on these mean scores, the code then calculates the number of tweets that are positive, negative, or neutral in sentiment.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntweet_sentences_data <- sentiment(get_sentences(biden_tweets$text)) %>% \n  group_by(element_id) %>% \n  summarize(meanSentiment = mean(sentiment))\n\n# Objects representing the number of positive, neutral, \n#and negative tweets from President Biden\nnegative_t <- sum(tweet_sentences_data$meanSentiment < 0)\nneutral_t <- sum(tweet_sentences_data$meanSentiment == 0)\npositive_t <- sum(tweet_sentences_data$meanSentiment > 0)\n\n# Creating vectors for a dataframe\ntype_tweet <- c(\"Negative\", \"Neutral\", \"Positive\")\nvalues <- c(negative_t, neutral_t, positive_t)\ndf_sentiment <- data.frame(type_tweet, values)\n```\n:::\n\n\n\n¬†\n\n# Sentiment Analysis of Biden's Tweets\n\nFinally, the results are visualized in a bar chart using the ggplot2 package.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ncustom_palette <- c(\"#FF6361\",\"#9B9B9B\", \"#00A693\")\n\nggplot(data = df_sentiment, aes(x = type_tweet, y = values, fill = type_tweet)) +\n  geom_col(width = 0.7, position = \"dodge\") +\n  scale_fill_manual(values = custom_palette) +\n  labs(title = \"Sentiment Analysis of Biden's Tweets\", x = NULL, y = \"Number of Tweets\") +\n  theme_minimal(base_size = 16) +\n  theme(plot.title = element_text(face = \"bold\", size = 20),\n        axis.title = element_text(size = 18),\n        axis.text = element_text(size = 14),\n        legend.position = \"none\",\n        panel.grid.major.y = element_line(color = \"gray80\"),\n        panel.grid.minor = element_blank())\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/fifth_plot-1.png){width=80%}\n:::\n:::\n\n\n¬†\n\n# Conclusion\n\n------------------------------------------------------------------------\n\nIn conclusion, social media scraping using R and Twitter is a powerful method for collecting and analyzing data from Twitter. The rtweet package in R provides a convenient and efficient way to access Twitter's REST API and retrieve large amounts of data, such as tweets, retweets, and user information.Using R and rtweet, researchers and data analysts can perform a wide range of analyses on Twitter data, including sentiment analysis, network analysis, and time series analysis. These analyses can provide insights into user behavior, sentiment, and trends on Twitter, which can be useful for a variety of applications, such as market research, political analysis, and social media monitoring.\n\n¬†\n\n# References\n\n------------------------------------------------------------------------\n\n::: {.callout-note}\nCite this page: Lehmann, L. (2023, April 18). *Introduction to Social Media Scraping*. Hertie Coding Club. [URL](https://www.hertiecodingclub.com/learn/rstudio/social_media_scraping/)\n:::",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}